{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Byeongali/CS50/blob/main/lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MBmOdXoenFw"
      },
      "source": [
        "# Lab 2: K-Nearest Neighbours\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3iS23ADfo4w"
      },
      "source": [
        "## Version history\n",
        "\n",
        "| Date | Author | Description |\n",
        "|:----:|:------:|:------------|\n",
        "2021-01-18 | Josiah Wang | First version | \n",
        "2021-10-18 | Josiah Wang | Fixed typo in Distance Weighted K-NN Classifier evaluation. The last piece of code should say WeightedKNNClassifier(k) and not KNNClassifier(k), obviously! |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EbbkgqOgZK_"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "The aim of this lab exercise is to give you some practical experience to build a K-Nearest Neighbours (K-NN) classifier.\n",
        "\n",
        "By the end of this lab exercise, you will have constructed different variants of a K-NN classifier as discussed in the lectures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1PH_TJZhf-c"
      },
      "source": [
        "## The Iris dataset\n",
        "\n",
        "Continuing where we left off in Lab 1, we will again work with the Iris dataset.\n",
        "\n",
        "I will just copy and rerun my solutions from Lab 1. Feel free to copy your own implementations over."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdL1OHXvifcA"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from numpy.random import default_rng\n",
        "\n",
        "# Download iris data if it does not exist\n",
        "if not os.path.exists(\"iris.data\"):\n",
        "    !wget -O iris.data https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n",
        "\n",
        "\n",
        "def read_dataset(filepath):\n",
        "    \"\"\" Read in the dataset from the specified filepath\n",
        "\n",
        "    Args:\n",
        "        filepath (str): The filepath to the dataset file\n",
        "\n",
        "    Returns:\n",
        "        tuple: returns a tuple of (x, y, classes), each being a numpy array. \n",
        "               - x is a numpy array with shape (N, K), \n",
        "                   where N is the number of instances\n",
        "                   K is the number of features/attributes\n",
        "               - y is a numpy array with shape (N, ), and should be integers from 0 to C-1\n",
        "                   where C is the number of classes \n",
        "               - classes : a numpy array with shape (C, ), which contains the \n",
        "                   unique class labels corresponding to the integers in y\n",
        "    \"\"\"\n",
        "\n",
        "    x = []\n",
        "    y_labels = []\n",
        "    for line in open(filepath):\n",
        "        if line.strip() != \"\": # handle empty rows in file\n",
        "            row = line.strip().split(\",\")\n",
        "            x.append(list(map(float, row[:-1]))) \n",
        "            y_labels.append(row[-1])\n",
        "    \n",
        "    [classes, y] = np.unique(y_labels, return_inverse=True) \n",
        "\n",
        "    x = np.array(x)\n",
        "    y = np.array(y)\n",
        "    return (x, y, classes)\n",
        "\n",
        "\n",
        "def split_dataset(x, y, test_proportion, random_generator=default_rng()):\n",
        "    \"\"\" Split dataset into training and test sets, according to the given \n",
        "        test set proportion.\n",
        "    \n",
        "    Args:\n",
        "        x (np.ndarray): Instances, numpy array with shape (N,K)\n",
        "        y (np.ndarray): Class labels, numpy array with shape (N,)\n",
        "        test_proprotion (float): the desired proportion of test examples \n",
        "                                 (0.0-1.0)\n",
        "        random_generator (np.random.Generator): A random generator\n",
        "\n",
        "    Returns:\n",
        "        tuple: returns a tuple of (x_train, x_test, y_train, y_test) \n",
        "               - x_train (np.ndarray): Training instances shape (N_train, K)\n",
        "               - x_test (np.ndarray): Test instances shape (N_test, K)\n",
        "               - y_train (np.ndarray): Training labels, shape (N_train, )\n",
        "               - y_test (np.ndarray): Test labels, shape (N_train, )\n",
        "    \"\"\"\n",
        "\n",
        "    shuffled_indices = random_generator.permutation(len(x))\n",
        "    n_test = round(len(x) * test_proportion)\n",
        "    n_train = len(x) - n_test\n",
        "    x_train = x[shuffled_indices[:n_train]]\n",
        "    y_train = y[shuffled_indices[:n_train]]\n",
        "    x_test = x[shuffled_indices[n_train:]]\n",
        "    y_test = y[shuffled_indices[n_train:]]\n",
        "    return (x_train, x_test, y_train, y_test)\n",
        "\n",
        "\n",
        "\n",
        "(x, y, classes) = read_dataset(\"iris.data\")\n",
        "\n",
        "seed = 60012\n",
        "rg = default_rng(seed)\n",
        "x_train, x_test, y_train, y_test = split_dataset(x, y, \n",
        "                                                 test_proportion=0.2, \n",
        "                                                 random_generator=rg)\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LNLsjjiwPJi"
      },
      "source": [
        "## K-NN Classifier\n",
        "\n",
        "In Lab 1, you have constructed a (one) Nearest Neighbour classifier.\n",
        "\n",
        "We will now try to generalise the nearest neighbour classifier as a **K-Nearest Neighours (K-NN)** classifier. For each test example, the classifier will predict the majority class label among the $K$ nearest training examples, again according to the Euclidean distance metric $d(x^{(i)}, x^{(q)})=\\sqrt{\\sum_f^F (x_f^{(i)} - x_f^{(q)})^2}$. If there is draw, choose one of the majority class labels arbitrarily, or at random.\n",
        "\n",
        "Complete the `predict()` method of the `KNNClassifier` class. Note that the class now takes an optional hyperparameter `k` in its constructor.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtnx_HjTyA2N"
      },
      "source": [
        "class KNNClassifier:\n",
        "    def __init__(self, k=5):\n",
        "        \"\"\" K-NN Classifier.\n",
        "\n",
        "        Args:\n",
        "        k (int): Number of nearest neighbours. Defaults to 5.\n",
        "        \"\"\"\n",
        "        self.k = k\n",
        "        self.x = np.array([])\n",
        "        self.y = np.array([])\n",
        "\n",
        "    def fit(self, x, y):\n",
        "        \"\"\" Fit the training data to the classifier.\n",
        "\n",
        "        Args:\n",
        "        x (np.ndarray): Instances, numpy array with shape (N,K)\n",
        "        y (np.ndarray): Class labels, numpy array with shape (N,)\n",
        "        \"\"\"\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "\n",
        "    def predict(self, x):\n",
        "        \"\"\" Perform prediction given some examples.\n",
        "\n",
        "        Args:\n",
        "        x (np.ndarray): Instances, numpy array with shape (N,K)\n",
        "\n",
        "        Returns:\n",
        "        y (np.ndarray): Predicted class labels, numpy array with shape (N,)\n",
        "        \"\"\" \n",
        "\n",
        "        # TODO: Implement a K-NN classifier\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBpJ8EjVy-bi"
      },
      "source": [
        "nn_classifier = KNNClassifier(k=5)\n",
        "nn_classifier.fit(x_train, y_train)\n",
        "nn_predictions = nn_classifier.predict(x_test)\n",
        "print(nn_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4yZIOgepd2L"
      },
      "source": [
        "### Evaluation\n",
        "\n",
        "Now, let's evaluate the accuracy of our K-NN classifier. We will test K from 1 to 20. Again, you should be able to achieve very high accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfjzLLwUqDC-"
      },
      "source": [
        "def compute_accuracy(y_gold, y_prediction):\n",
        "    \"\"\" Compute the accuracy given the ground truth and predictions\n",
        "\n",
        "    Args:\n",
        "    y_gold (np.ndarray): the correct ground truth/gold standard labels\n",
        "    y_prediction (np.ndarray): the predicted labels\n",
        "\n",
        "    Returns:\n",
        "    float : the accuracy\n",
        "    \"\"\"\n",
        "\n",
        "    assert len(y_gold) == len(y_prediction)  \n",
        "    \n",
        "    try:\n",
        "        return np.sum(y_gold == y_prediction) / len(y_gold)\n",
        "    except ZeroDivisionError:\n",
        "        return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWRWsYurz1om"
      },
      "source": [
        "for k in range(1, 21):\n",
        "    nn_classifier = KNNClassifier(k)\n",
        "    nn_classifier.fit(x_train, y_train)\n",
        "    nn_predictions = nn_classifier.predict(x_test)\n",
        "    accuracy = compute_accuracy(y_test, nn_predictions)\n",
        "    print(f\"K={k}: {accuracy}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9z8I2TXaLepZ"
      },
      "source": [
        "## Distance Weighted K-NN Classifier\n",
        "\n",
        "Now, let us implement the **distance weighted** K-NN classifier as discussed in the lectures. \n",
        "Pleae complete the `predict()` method for the `WeightedKNNClassifier` class below.\n",
        "\n",
        "For each test example $x^{(q)}$, you will need to compute the weight $w^{(i)}_{q}$ for each training example $x^{(i)}$. \n",
        "\n",
        "Then for each class label, sum the weights of the $K$ nearest examples. Assign the test example to the class label with largest sum.\n",
        "\n",
        "The weight can be anything reasonable. For this tutorial, we will use the weight $w_{q}^{(i)}=\\frac{1}{d(x^{(i)}, x^{(q)})}$, where $d(x^{(i)}, x^{(q)})=\\sqrt{\\sum_f^F (x_f^{(i)} - x_f^{(q)})^2}$ is the Euclidean distance. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTQ-cgA8LTFB"
      },
      "source": [
        "class WeightedKNNClassifier:\n",
        "    def __init__(self, k=5):\n",
        "        \"\"\" K-NN Classifier.\n",
        "\n",
        "        Args:\n",
        "        k (int): Number of nearest neighbours. Defaults to 5.\n",
        "        \"\"\"\n",
        "        self.k = k\n",
        "        self.x = np.array([])\n",
        "        self.y = np.array([])\n",
        "\n",
        "    def fit(self, x, y):\n",
        "        \"\"\" Fit the training data to the classifier.\n",
        "\n",
        "        Args:\n",
        "        x (np.ndarray): Instances, numpy array with shape (N,K)\n",
        "        y (np.ndarray): Class labels, numpy array with shape (N,)\n",
        "        \"\"\"\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "\n",
        "    def predict(self, x):\n",
        "        \"\"\" Perform prediction given some examples.\n",
        "\n",
        "        Args:\n",
        "        x (np.ndarray): Instances, numpy array with shape (N,K)\n",
        "\n",
        "        Returns:\n",
        "        y (np.ndarray): Predicted class labels, numpy array with shape (N,)\n",
        "        \"\"\" \n",
        "\n",
        "        # TODO: Implement a distance weighted K-NN classifier\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gjm_y7NnLTFP"
      },
      "source": [
        "nn_classifier = WeightedKNNClassifier(k=5)\n",
        "nn_classifier.fit(x_train, y_train)\n",
        "nn_predictions = nn_classifier.predict(x_test)\n",
        "print(nn_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCfaQ5WmgqSa"
      },
      "source": [
        "### Evaluation\n",
        "\n",
        "Again, we will evaluate the accuracy of the distance weighted K-NN classifier for K=1 to 20. As this is a small and relatively easy dataset, you should get similar results as with the simple K-NN classifier.\n",
        "\n",
        "You might observe the benefits of the distance weighted K-NN classifier better when training on larger and noisier datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPr1YEfJ3g8Z"
      },
      "source": [
        "for k in range(1, 21):\n",
        "    nn_classifier = WeightedKNNClassifier(k)\n",
        "    nn_classifier.fit(x_train, y_train)\n",
        "    nn_predictions = nn_classifier.predict(x_test)\n",
        "    accuracy = compute_accuracy(y_test, nn_predictions)\n",
        "    print(f\"K={k}: {accuracy}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNMajcyH7Vv5"
      },
      "source": [
        "## Summary\n",
        "\n",
        "You have built a K-Nearest Neighbour classifier (and the distance-weighted variant)! Because the Iris dataset is a small and simple dataset, you may only observe a small improvement in accuracy (if any) compared to a simple one-nearest neighbour classifier.\n",
        "\n",
        "And that is it for this week's lab tutorials. We will not have a decision tree tutorial for this course. The coursework itself will contain a short guide to help you think about implementing your own decision trees.\n",
        "\n",
        "In the next lab tutorial, you will be implementing different evaluation metrics and performing cross-validation.\n"
      ]
    }
  ]
}