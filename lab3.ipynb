{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Byeongali/CS50/blob/main/lab3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MBmOdXoenFw"
      },
      "source": [
        "# Lab 3: Machine Learning Evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3iS23ADfo4w"
      },
      "source": [
        "## Version history\n",
        "\n",
        "| Date | Author | Description |\n",
        "|:----:|:------:|:------------|\n",
        "2021-01-27 | Josiah Wang | First version | \n",
        "2021-10-24 | Josiah Wang | Added missing random generator parameter/arguments to cross validation functions/function calls |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EbbkgqOgZK_"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "The aim of this lab exercise is to improve your understanding and to give you some practical experience in evaluating machine learning algorithms.\n",
        "\n",
        "By the end of this lab exercise, you will have \n",
        "- implemented different evaluation metrics\n",
        "- performed cross-validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1PH_TJZhf-c"
      },
      "source": [
        "## Setup\n",
        "\n",
        "To work on this lab, we will copy some of our solutions from the previous labs. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-Gz83kV8H3D"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "We will again work with the Iris dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdL1OHXvifcA"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from numpy.random import default_rng\n",
        "\n",
        "# Download iris data if it does not exist\n",
        "if not os.path.exists(\"iris.data\"):\n",
        "    !wget -O iris.data https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n",
        "\n",
        "\n",
        "def read_dataset(filepath):\n",
        "    \"\"\" Read in the dataset from the specified filepath\n",
        "\n",
        "    Args:\n",
        "        filepath (str): The filepath to the dataset file\n",
        "\n",
        "    Returns:\n",
        "        tuple: returns a tuple of (x, y, classes), each being a numpy array. \n",
        "               - x is a numpy array with shape (N, K), \n",
        "                   where N is the number of instances\n",
        "                   K is the number of features/attributes\n",
        "               - y is a numpy array with shape (N, ), and should be integers from 0 to C-1\n",
        "                   where C is the number of classes \n",
        "               - classes : a numpy array with shape (C, ), which contains the \n",
        "                   unique class labels corresponding to the integers in y\n",
        "    \"\"\"\n",
        "\n",
        "    x = []\n",
        "    y_labels = []\n",
        "    for line in open(filepath):\n",
        "        if line.strip() != \"\": # handle empty rows in file\n",
        "            row = line.strip().split(\",\")\n",
        "            x.append(list(map(float, row[:-1]))) \n",
        "            y_labels.append(row[-1])\n",
        "    \n",
        "    [classes, y] = np.unique(y_labels, return_inverse=True) \n",
        "\n",
        "    x = np.array(x)\n",
        "    y = np.array(y)\n",
        "    return (x, y, classes)\n",
        "\n",
        "\n",
        "def split_dataset(x, y, test_proportion, random_generator=default_rng()):\n",
        "    \"\"\" Split dataset into training and test sets, according to the given \n",
        "        test set proportion.\n",
        "    \n",
        "    Args:\n",
        "        x (np.ndarray): Instances, numpy array with shape (N,K)\n",
        "        y (np.ndarray): Class labels, numpy array with shape (N,)\n",
        "        test_proprotion (float): the desired proportion of test examples \n",
        "                                 (0.0-1.0)\n",
        "        random_generator (np.random.Generator): A random generator\n",
        "\n",
        "    Returns:\n",
        "        tuple: returns a tuple of (x_train, x_test, y_train, y_test) \n",
        "               - x_train (np.ndarray): Training instances shape (N_train, K)\n",
        "               - x_test (np.ndarray): Test instances shape (N_test, K)\n",
        "               - y_train (np.ndarray): Training labels, shape (N_train, )\n",
        "               - y_test (np.ndarray): Test labels, shape (N_test, )\n",
        "    \"\"\"\n",
        "\n",
        "    shuffled_indices = random_generator.permutation(len(x))\n",
        "    n_test = round(len(x) * test_proportion)\n",
        "    n_train = len(x) - n_test\n",
        "    x_train = x[shuffled_indices[:n_train]]\n",
        "    y_train = y[shuffled_indices[:n_train]]\n",
        "    x_test = x[shuffled_indices[n_train:]]\n",
        "    y_test = y[shuffled_indices[n_train:]]\n",
        "    return (x_train, x_test, y_train, y_test)\n",
        "\n",
        "\n",
        "\n",
        "(x, y, classes) = read_dataset(\"iris.data\")\n",
        "\n",
        "seed = 60012\n",
        "rg = default_rng(seed)\n",
        "x_train, x_test, y_train, y_test = split_dataset(x, y, \n",
        "                                                 test_proportion=0.2, \n",
        "                                                 random_generator=rg)\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AsfLi6x8Vhj"
      },
      "source": [
        "### Classifier\n",
        "\n",
        "We will also evaluate our random baseline classifier from Lab 1 and the K-nearest neighbours classifier from Lab 2. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bo11NdHp8-Tk"
      },
      "source": [
        "class RandomClassifier:\n",
        "    def __init__(self, random_generator=default_rng()):\n",
        "        self.random_generator = random_generator\n",
        "        self.unique_y = [] \n",
        "\n",
        "    def fit(self, x, y):\n",
        "        \"\"\" Fit the training data to the classifier.\n",
        "\n",
        "        Args:\n",
        "            x (np.ndarray): Instances, numpy array with shape (N,K)\n",
        "            y (np.ndarray): Class labels, numpy array with shape (N,)\n",
        "        \"\"\"\n",
        "        self.unique_y = list(set(y))\n",
        "\n",
        "    def predict(self, x):\n",
        "        \"\"\" Perform prediction given some examples.\n",
        "\n",
        "        Args:\n",
        "            x (np.ndarray): Instances, numpy array with shape (N,K)\n",
        "\n",
        "        Returns:\n",
        "            y (np.ndarray): Predicted class labels, numpy array with shape (N,)\n",
        "        \"\"\"        \n",
        "        random_indices = self.random_generator.integers(0, len(self.unique_y), len(x))\n",
        "        y = np.array(self.unique_y)\n",
        "        return y[random_indices]\n",
        "\n",
        "\n",
        "random_classifier = RandomClassifier(rg)\n",
        "random_classifier.fit(x_train, y_train)\n",
        "random_predictions = random_classifier.predict(x_test)\n",
        "print(random_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtnx_HjTyA2N"
      },
      "source": [
        "class KNNClassifier:\n",
        "    def __init__(self, k=5):\n",
        "        \"\"\" K-NN Classifier.\n",
        "\n",
        "        Args:\n",
        "            k (int): Number of nearest neighbours. Defaults to 5.\n",
        "        \"\"\"\n",
        "        self.k = k\n",
        "        self.x = np.array([])\n",
        "        self.y = np.array([])\n",
        "\n",
        "    def fit(self, x, y):\n",
        "        \"\"\" Fit the training data to the classifier.\n",
        "\n",
        "        Args:\n",
        "            x (np.ndarray): Instances, numpy array with shape (N,K)\n",
        "            y (np.ndarray): Class labels, numpy array with shape (N,)\n",
        "        \"\"\"\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "\n",
        "    def predict(self, x):\n",
        "        \"\"\" Perform prediction given some examples.\n",
        "\n",
        "        Args:\n",
        "            x (np.ndarray): Instances, numpy array with shape (N,K)\n",
        "\n",
        "        Returns:\n",
        "            y (np.ndarray): Predicted class labels, numpy array with shape (N,)\n",
        "        \"\"\" \n",
        "\n",
        "        # just to make sure that we have enough training examples!\n",
        "        k = min([self.k, len(self.x)])\n",
        "\n",
        "        y = np.zeros((len(x), ), dtype=self.y.dtype)       \n",
        "        for (i, instance) in enumerate(x):\n",
        "            distances = np.sqrt(np.sum((instance-self.x)**2, axis=1))\n",
        "            sorted_indices = np.argsort(distances)\n",
        "            sorted_indices = sorted_indices[:k]\n",
        "            \n",
        "            # Assign to the majority class label (the mode)\n",
        "            unique_labels, freq = np.unique(self.y[sorted_indices], return_counts=True)\n",
        "            y[i] = unique_labels[freq.argmax()]\n",
        "\n",
        "        return y\n",
        "\n",
        "\n",
        "knn_classifier = KNNClassifier(k=1) # we'll do one nearest neighbour\n",
        "knn_classifier.fit(x_train, y_train)\n",
        "knn_predictions = knn_classifier.predict(x_test)\n",
        "print(knn_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4yZIOgepd2L"
      },
      "source": [
        "## Evaluation metrics\n",
        "\n",
        "Now, let us implement some functions to compute some of the metrics discussed in the lectures. For simplicity, in this tutorial we will write separate functions for each, but you can also write them as methods for a single class, for example an `Evaluator` class. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yVR-e6eAhA8"
      },
      "source": [
        "### Confusion Matrix\n",
        "\n",
        "Firstly, given a list of predictions and the ground truth annotations, complete the function `confusion_matrix()` to compute the confusion matrix. While not a metric in itself, the confusion matrix is a useful visualisation tool for analysing the performance of your classifier across categories. You can also compute many evaluation metrics from the confusion matrix.\n",
        "\n",
        "We will use the standard convention as shown in the lectures. The rows are the correct classes, the columns are the predicted classes.\n",
        "\n",
        "Implementation-wise, for each of the correct classes (row), you should count how many of these instances are predicted for each class.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClFUgP_vBL48"
      },
      "source": [
        "def confusion_matrix(y_gold, y_prediction, class_labels=None):\n",
        "    \"\"\" Compute the confusion matrix.\n",
        "        \n",
        "    Args:\n",
        "        y_gold (np.ndarray): the correct ground truth/gold standard labels\n",
        "        y_prediction (np.ndarray): the predicted labels\n",
        "        class_labels (np.ndarray): a list of unique class labels. \n",
        "                               Defaults to the union of y_gold and y_prediction.\n",
        "\n",
        "    Returns:\n",
        "        np.array : shape (C, C), where C is the number of classes. \n",
        "                   Rows are ground truth per class, columns are predictions\n",
        "    \"\"\"\n",
        "\n",
        "    # if no class_labels are given, we obtain the set of unique class labels from\n",
        "    # the union of the ground truth annotation and the prediction\n",
        "    if not class_labels:\n",
        "        class_labels = np.unique(np.concatenate((y_gold, y_prediction)))\n",
        "\n",
        "    confusion = np.zeros((len(class_labels), len(class_labels)), dtype=np.int)\n",
        "\n",
        "    # TODO: Complete this\n",
        "    # for each correct class (row), \n",
        "    # compute how many instances are predicted for each class (columns)\n",
        "\n",
        "\n",
        "    return confusion\n",
        "\n",
        "\n",
        "\n",
        "# Compute confusion on predictions for RandomClassifier and KNNClassifier from earlier\n",
        "print(\"Ground truth:\", y_test)\n",
        "\n",
        "print(\"Random:\", random_predictions)\n",
        "confusion_random = confusion_matrix(y_test, random_predictions)\n",
        "print(confusion_random)\n",
        "\n",
        "print(\"KNN:\", knn_predictions)\n",
        "confusion_knn = confusion_matrix(y_test, knn_predictions)\n",
        "print(confusion_knn)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcotIhFPeaFk"
      },
      "source": [
        "### Accuracy\n",
        "\n",
        "We have already implemented the accuracy metric in Lab 1. I have copied it and renamed it to `accuracy()` just to make it more compact."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfjzLLwUqDC-"
      },
      "source": [
        "def accuracy(y_gold, y_prediction):\n",
        "    \"\"\" Compute the accuracy given the ground truth and predictions\n",
        "\n",
        "    Args:\n",
        "        y_gold (np.ndarray): the correct ground truth/gold standard labels\n",
        "        y_prediction (np.ndarray): the predicted labels\n",
        "\n",
        "    Returns:\n",
        "        float : the accuracy\n",
        "    \"\"\"\n",
        "\n",
        "    assert len(y_gold) == len(y_prediction)  \n",
        "    \n",
        "    try:\n",
        "        return np.sum(y_gold == y_prediction) / len(y_gold)\n",
        "    except ZeroDivisionError:\n",
        "        return 0.\n",
        "\n",
        "# Compute accuracy on predictions for RandomClassifier and KNNClassifier from earlier\n",
        "print(accuracy(y_test, random_predictions))\n",
        "print(accuracy(y_test, knn_predictions))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDxicv6re1yW"
      },
      "source": [
        "As an extra exercise, you can also compute the accuracy directly from a confusion matrix. This is just a different way of computing the accuracy. You can actually do this in one single line (if you disregard division by zero cases). Hint: you might have already come across the solution in my NumPy tutorial. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntWntjmhgnZ6"
      },
      "source": [
        "def accuracy_from_confusion(confusion):\n",
        "    \"\"\" Compute the accuracy given the confusion matrix\n",
        "\n",
        "    Args:\n",
        "        confusion (np.ndarray): shape (C, C), where C is the number of classes. \n",
        "                    Rows are ground truth per class, columns are predictions\n",
        "\n",
        "    Returns:\n",
        "        float : the accuracy\n",
        "    \"\"\"   \n",
        "\n",
        "    if np.sum(confusion) > 0:\n",
        "        # TODO: Complete this\n",
        "        return ???? \n",
        "    else:\n",
        "        return 0."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywo-pHeHtPYE"
      },
      "source": [
        "And let's test our implementation (and check that the accuracy matches the one from above)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81pUYiS5tRsC"
      },
      "source": [
        "print(accuracy_from_confusion(confusion_random))\n",
        "print(accuracy_from_confusion(confusion_knn))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOE9vH7ykj1D"
      },
      "source": [
        "### Precision\n",
        "\n",
        "Now, complete the function `precision()` to compute the **precision** given the predictions and the ground truth annotations. \n",
        "\n",
        "To complement the equation given in the lecture, I personally find it easier to use this statement to remember the definition of precision: \"*out of all instances predicted as positive, how many are correctly predicted?*\" The second half of the statement is the numerator, and the first half is the denominator.\n",
        "\n",
        "You should compute one precision score per class (so  3 scores for the Iris dataset). You might as well also compute and return the macro-averaged precision score at the same time. Also remember to account for division-by-zero cases -- for this lab we will return 0 if the denominator is 0 (Some implementations return `NaN` or `undefined`).\n",
        "\n",
        "It is probably easier to first compute the confusion matrix and then compute the precision from the matrix (although you do not have to do this).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULrjLMz8ldQc"
      },
      "source": [
        "def precision(y_gold, y_prediction):\n",
        "    \"\"\" Compute the precision score per class given the ground truth and predictions\n",
        "        \n",
        "    Also return the macro-averaged precision across classes.\n",
        "        \n",
        "    Args:\n",
        "        y_gold (np.ndarray): the correct ground truth/gold standard labels\n",
        "        y_prediction (np.ndarray): the predicted labels\n",
        "\n",
        "    Returns:\n",
        "        tuple: returns a tuple (precisions, macro_precision) where\n",
        "            - precisions is a np.ndarray of shape (C,), where each element is the \n",
        "              precision for class c\n",
        "            - macro-precision is macro-averaged precision (a float) \n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: Complete this function\n",
        "\n",
        "    # Compute the precision per class\n",
        "    p = ????\n",
        "    \n",
        "    # Compute the macro-averaged precision\n",
        "    macro_p = ????\n",
        "    \n",
        "    return (p, macro_p)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_j4Mj6RrvS0K"
      },
      "source": [
        "Test your function..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtfIs8KnvR5R"
      },
      "source": [
        "(p_random, macro_p_random) = precision(y_test, random_predictions)\n",
        "print(p_random)\n",
        "print(macro_p_random)\n",
        "\n",
        "(p_knn, macro_p_knn) = precision(y_test, knn_predictions)\n",
        "print(p_knn)\n",
        "print(macro_p_knn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewCJAUJMzUhj"
      },
      "source": [
        "### Recall\n",
        "\n",
        "The next metric is **recall**. Complete the function `recall()` to compute the per-class recall (and macro-averaged recall) given the predictions and the ground truth annotations.\n",
        "\n",
        "Again, to remember the definition of recall, I personally use this statement: \"*out of all instances that are actually positive, how many are correctly retrieved?*\"\n",
        "\n",
        "This is just a small modification from computing precision. You really only need to change the denominator. So you can actually copy your solution from precision and do a few small tweaks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rKePOBL0jb_"
      },
      "source": [
        "def recall(y_gold, y_prediction):\n",
        "    \"\"\" Compute the recall score per class given the ground truth and predictions\n",
        "        \n",
        "    Also return the macro-averaged recall across classes.\n",
        "        \n",
        "    Args:\n",
        "        y_gold (np.ndarray): the correct ground truth/gold standard labels\n",
        "        y_prediction (np.ndarray): the predicted labels\n",
        "\n",
        "    Returns:\n",
        "        tuple: returns a tuple (recalls, macro_recall) where\n",
        "            - recalls is a np.ndarray of shape (C,), where each element is the \n",
        "                recall for class c\n",
        "            - macro-recall is macro-averaged recall (a float) \n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: Complete this function\n",
        "\n",
        "    # Compute the recall per class\n",
        "    r = ????\n",
        "    \n",
        "    # Compute the macro-averaged recall\n",
        "    macro_r = ????\n",
        "    \n",
        "    return (r, macro_r)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "es0jJd6V1ieB"
      },
      "source": [
        "Again, test your function..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqZVGCOX1mfy"
      },
      "source": [
        "(r_random, macro_r_random) = recall(y_test, random_predictions)\n",
        "print(r_random)\n",
        "print(macro_r_random)\n",
        "\n",
        "(r_knn, macro_r_knn) = recall(y_test, knn_predictions)\n",
        "print(r_knn)\n",
        "print(macro_r_knn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aar63JOT2rC5"
      },
      "source": [
        "### $F_1$-score\n",
        "\n",
        "Finally, let us compute the $F_1$-score (or $F_1$-measure). Since you have already implemented `precision()` and `recall()`, this function should be a piece of cake! \n",
        "\n",
        "The macro-averaged $F_1$ is worth a bit more discussion though. There are actually **two** possible definitions of macro-averaged $F_1$ (although most people do not think about it).\n",
        "1. Compute the mean of $F_1$ scores across the classes; or\n",
        "2. Compute the *harmonic mean* between the *macro-averaged precision* and *macro-average recall*\n",
        "\n",
        "Both definitions are used in the literature, and both give different values. See https://towardsdatascience.com/a-tale-of-two-macro-f1s-8811ddcf8f04 for a detailed discussion if you are interested.\n",
        "\n",
        "For this lab (and our course), we will use the **first** definition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLOISUzl3dOx"
      },
      "source": [
        "def f1_score(y_gold, y_prediction):\n",
        "    \"\"\" Compute the F1-score per class given the ground truth and predictions\n",
        "        \n",
        "    Also return the macro-averaged F1-score across classes.\n",
        "        \n",
        "    Args:\n",
        "        y_gold (np.ndarray): the correct ground truth/gold standard labels\n",
        "        y_prediction (np.ndarray): the predicted labels\n",
        "\n",
        "    Returns:\n",
        "        tuple: returns a tuple (f1s, macro_f1) where\n",
        "            - f1s is a np.ndarray of shape (C,), where each element is the \n",
        "              f1-score for class c\n",
        "            - macro-f1 is macro-averaged f1-score (a float) \n",
        "    \"\"\"\n",
        "\n",
        "    (precisions, macro_p) = precision(y_gold, y_prediction)\n",
        "    (recalls, macro_r) = recall(y_gold, y_prediction)\n",
        "\n",
        "    # just to make sure they are of the same length\n",
        "    assert len(precisions) == len(recalls)\n",
        "\n",
        "    # TODO: Complete this to compute the per-class F1\n",
        "    f = np.zeros((len(precisions), ))\n",
        "\n",
        "\n",
        "    # TODO: Compute the macro-averaged F1\n",
        "    macro_f = ???\n",
        "    \n",
        "    return (f, macro_f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUHHvdbG6goe"
      },
      "source": [
        "And we test our function as usual."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tufC6526koh"
      },
      "source": [
        "(f1_random, macro_f1_random) = f1_score(y_test, random_predictions)\n",
        "print(f1_random)\n",
        "print(macro_f1_random)\n",
        "\n",
        "(f1_knn, macro_f1_knn) = f1_score(y_test, knn_predictions)\n",
        "print(f1_knn)\n",
        "print(macro_f1_knn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70KZOlMNJfe5"
      },
      "source": [
        "And that is all the metrics that you will implement! You can use these for your first coursework to evaluate your classifiers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNIqUTkR6-LI"
      },
      "source": [
        "## Cross-validation\n",
        "\n",
        "So far, we have evaluated our classifiers by splitting our dataset into a *training set* and a *test set*. \n",
        "\n",
        "One of the issues with using a *single* test set is that it is sometimes difficult to definitively conclude that one model  is better than another. Your model might just so happen to perform better than another on one particular test set, and\n",
        "the opposite might happen if evaluated on another. Therefore, **k-fold cross-validation** is commonly\n",
        "performed to ensure that the better performance is not just by chance, but is consistent across different\n",
        "data splits. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_VSMVBLoDf_"
      },
      "source": [
        "### Divide dataset into $k$ splits\n",
        "\n",
        "Below is a utility function to randomly divide a dataset into $k$ approximately equal-sized splits so that we can use these splits for cross-validation.\n",
        "\n",
        "The function will return the *indices* rather than the dataset itself. Otherwise you will have to split `x` and `y` simultaneously and make sure they correspond to each other correctly. We can later access the appropriate `x` and `y` using these indices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjRpkpUFnZbQ"
      },
      "source": [
        "def k_fold_split(n_splits, n_instances, random_generator=default_rng()):\n",
        "    \"\"\" Split n_instances into n mutually exclusive splits at random.\n",
        "    \n",
        "    Args:\n",
        "        n_splits (int): Number of splits\n",
        "        n_instances (int): Number of instances to split\n",
        "        random_generator (np.random.Generator): A random generator\n",
        "\n",
        "    Returns:\n",
        "        list: a list (length n_splits). Each element in the list should contain a \n",
        "            numpy array giving the indices of the instances in that split.\n",
        "    \"\"\"\n",
        "\n",
        "    # generate a random permutation of indices from 0 to n_instances\n",
        "    shuffled_indices = random_generator.permutation(n_instances)\n",
        "\n",
        "    # split shuffled indices into almost equal sized splits\n",
        "    split_indices = np.array_split(shuffled_indices, n_splits)\n",
        "\n",
        "    return split_indices\n",
        "\n",
        "# For quick testing\n",
        "k_fold_split(3, 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVwflah7_DCQ"
      },
      "source": [
        "### $k$-fold cross validation\n",
        "\n",
        "Now, complete the `train_test_k_fold` function below. This function will generate indices for train and test splits for each fold. For example, if there are 20 instances and 3 folds, you should first divide the 20 instances into 3 splits:\n",
        "\n",
        "```python\n",
        "[array([19,  6, 14, 13, 17, 12,  5]),\n",
        " array([ 1,  2,  0, 10, 18, 15, 16]),\n",
        " array([ 4,  3,  8,  9,  7, 11])]\n",
        " ```\n",
        "\n",
        "You will then produce 3 train/test splits for each fold, using one of the splits for testing each time, and the remaining two splits for training. For example:\n",
        "\n",
        "```python\n",
        "[[array([ 1,  2,  0, 10, 18, 15, 16, 4,  3,  8,  9,  7, 11]), array([19,  6, 14, 13, 17, 12,  5])]\n",
        " [array([19,  6, 14, 13, 17, 12,  5, 4,  3,  8,  9,  7, 11]), array([1,  2,  0, 10, 18, 15, 16])]\n",
        " [array([19,  6, 14, 13, 17, 12,  5, 1,  2,  0, 10, 18, 15]), array([16, 4,  3,  8,  9,  7, 11])]\n",
        "]\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vN-FatHt1mvQ"
      },
      "source": [
        "def train_test_k_fold(n_folds, n_instances, random_generator=default_rng()):\n",
        "    \"\"\" Generate train and test indices at each fold.\n",
        "    \n",
        "    Args:\n",
        "        n_folds (int): Number of folds\n",
        "        n_instances (int): Total number of instances\n",
        "        random_generator (np.random.Generator): A random generator\n",
        "\n",
        "    Returns:\n",
        "        list: a list of length n_folds. Each element in the list is a list (or tuple) \n",
        "            with two elements: a numpy array containing the train indices, and another \n",
        "            numpy array containing the test indices.\n",
        "    \"\"\"\n",
        "\n",
        "    # split the dataset into k splits\n",
        "    split_indices = k_fold_split(n_folds, n_instances, random_generator)\n",
        "\n",
        "    folds = []\n",
        "    for k in range(n_folds):\n",
        "        # TODO: Complete this\n",
        "        # take the splits from split_indices and keep the k-th split as testing\n",
        "        # and concatenate the remaining splits for training\n",
        "\n",
        "        test_indices = ????\n",
        "        train_indices = ????\n",
        "\n",
        "        folds.append([train_indices, test_indices])\n",
        "\n",
        "    return folds\n",
        "\n",
        "\n",
        "# to test your function (30 instances, 4 fold)\n",
        "for (train_indices, test_indices) in train_test_k_fold(4, 30, rg):\n",
        "    print(\"train: \", train_indices)\n",
        "    print(\"test: \", test_indices)\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2XBAxWXlQom"
      },
      "source": [
        "You will now train and evaluate the K-NN classifier with 10-fold cross validation, using the original (unsplit) dataset (i.e. `x`, `y`). Compute the **accuracy** for each fold, and return the average accuracy and standard deviation across all folds. You will likely see that some of the accuracies are higher than others. You can set the number of nearest neighbours for the K-NN classifier to be any number."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VK-IL_qbm9B9"
      },
      "source": [
        "n_folds = 10\n",
        "accuracies = np.zeros((n_folds, ))\n",
        "for i, (train_indices, test_indices) in enumerate(train_test_k_fold(n_folds, len(x), rg)):\n",
        "    # Get the dataset from the correct splits\n",
        "    # TODO: Complete test\n",
        "    x_train = x[????, :]\n",
        "    y_train = y[????]\n",
        "    x_test = x[????, :]\n",
        "    y_test = y[????]\n",
        "\n",
        "    # TODO: Train the K-NN\n",
        "\n",
        "    # TODO: Predict on x_test\n",
        "\n",
        "    # TODO Compute accuracy\n",
        "\n",
        "    accuracies[i] = ????\n",
        "\n",
        "print(accuracies)\n",
        "print(accuracies.mean())\n",
        "print(accuracies.std())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKo-TQtcSxFs"
      },
      "source": [
        "### $k$-fold cross validation with hyperparameter tuning\n",
        "\n",
        "If your classifier has some hyperparameters (e.g. the number of neighbours $K$ for K-NN), you can divide your dataset into three disjoint sets: *training*, *validation* and *test*, and use the validation set to select the optimal hyperparamter value ($K$ in the case of K-NN). We will not implement this in this tutorial since this is a straightforward modification of the `split_dataset()` function from Lab 1 (try this yourself if you are so inclined!)\n",
        "\n",
        "We will instead focus on selecting the optimal hyperparameter values using $k$-fold cross-validation. This will ensure that you do not overestimate the accuracy of your algorithm. More specifically, you will implement cross-validation that includes a validation dataset as presented in the lecture. You will implement the version labelled as **Option 1**, which is much simpler than Option 2. To do this, for each fold, keep one split for testing, one split for validation, and the remaining splits for training. \n",
        "\n",
        "For this, you will need to first modify the `train_test_k_fold()` function from earlier. Call the function `train_val_test_k_fold()`, and it should now return the train, validation and test splits per fold instead of just train and test. For each fold, reserve one split as the test set, pick any one of the other splits as the validation set, and put the remaining splits together as the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-fIPCF7XkNl"
      },
      "source": [
        "def train_val_test_k_fold(n_folds, n_instances, random_generator=default_rng()):\n",
        "    \"\"\" Generate train and test indices at each fold.\n",
        "    \n",
        "    Args:\n",
        "        n_folds (int): Number of folds\n",
        "        n_instances (int): Total number of instances\n",
        "        random_generator (np.random.Generator): A random generator\n",
        "\n",
        "    Returns:\n",
        "        list: a list of length n_folds. Each element in the list is a list (or tuple) \n",
        "            with three elements: \n",
        "            - a numpy array containing the train indices\n",
        "            - a numpy array containing the val indices \n",
        "            - a numpy array containing the test indices\n",
        "    \"\"\"\n",
        "\n",
        "    # split the dataset into k splits\n",
        "    split_indices = k_fold_split(n_folds, n_instances, random_generator)\n",
        "\n",
        "    folds = []\n",
        "    for k in range(n_folds):\n",
        "        # TODO: Complete this\n",
        "        # take the splits from split_indices and keep the k-th split as testing\n",
        "        # and another split as validation\n",
        "        # and concatenate the remaining k-2 splits for training\n",
        "\n",
        "        test_indices = ????\n",
        "        val_indices = ????\n",
        "        train_indices = ????\n",
        "\n",
        "        folds.append([train_indices, val_indices, test_indices])\n",
        "        \n",
        "    return folds\n",
        "\n",
        "\n",
        "# to test your function (30 instances, 4 fold)\n",
        "for (train_indices, val_indices, test_indices) in train_val_test_k_fold(4, 30, rg):\n",
        "    print(\"train: \", train_indices)\n",
        "    print(\"validation: \", val_indices)    \n",
        "    print(\"test: \", test_indices)\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W83q9lxndntF"
      },
      "source": [
        "We will now implement cross-validation with hyperparameter tuning. For each fold, you will perform what is known as a **grid search** to exhaustively search for the hyperparameter value that optimises the performance of the model on the validation set. You should then evaluate this model on the held-out test set, and average the score across folds.\n",
        "\n",
        "For this exercise, you will try to optimise the **number of nearest neighbours** in a K-NN classifier, using the **accuracy** metric. For simplicity, just perform a grid search over 1 to 10 nearest neighbours. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ay_Iv-5ndoQN"
      },
      "source": [
        "n_folds = 10\n",
        "accuracies = np.zeros((n_folds, ))\n",
        "for i, (train_indices, val_indices, test_indices) in enumerate(train_val_test_k_fold(n_folds, len(x), rg)):\n",
        "    # set up the dataset for this fold\n",
        "    x_train = x[train_indices, :]\n",
        "    y_train = y[train_indices]\n",
        "    x_val = x[val_indices, :]\n",
        "    y_val = y[val_indices]\n",
        "    x_test = x[test_indices, :]\n",
        "    y_test = y[test_indices]\n",
        "\n",
        "    # TODO:\n",
        "    # Perform grid search, i.e.\n",
        "    # for K (number of neighbours) from 1 to 10 (inclusive)\n",
        "    #     evaluate the K-NN classifiers on x_val\n",
        "    #     store the accuracy and classifier for each K \n",
        "    # Select the classifier with the highest accuracy\n",
        "    # and evaluate this classifier on x_test (accuracy)\n",
        "\n",
        "    accuracies[i] = ????\n",
        "\n",
        "\n",
        "print(accuracies)\n",
        "print(accuracies.mean())\n",
        "print(accuracies.std())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4TtXW98ogYN"
      },
      "source": [
        "And that is it for cross-validation. \"Option 2\" from the lecture (nested cross-validation) is less prone to overfitting, but is much slower and much more complicated to implement, so we will not attempt it for this lab tutorial. There are multiple ways to design this; I will provide my implementation for Option 2 in the solutions, so that you can have a better understanding of it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNMajcyH7Vv5"
      },
      "source": [
        "## Summary\n",
        "\n",
        "Phew! This was an intense exercise. Congratulations! You have managed to implement most of the major metrics for classification. You have also implemented a simple form of cross-validation (with only training and testing splits) and performed hyperparameter tuning using cross-validation. \n",
        "\n",
        "You have hopefully gained a more in-depth understanding of cross-validation and the metrics. In practice, these can be quite tricky to implement from scratch (especially cross-validation), so you can just use the utilities provided by the scikit-learn library to easily perform cross-validation and grid search:\n",
        "https://scikit-learn.org/stable/modules/cross_validation.html \n"
      ]
    }
  ]
}