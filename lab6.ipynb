{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Byeongali/CS50/blob/main/lab6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MBmOdXoenFw"
      },
      "source": [
        "# Lab 6: Unsupervised Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3iS23ADfo4w"
      },
      "source": [
        "## Version history\n",
        "\n",
        "| Date | Author | Description |\n",
        "|:----:|:------:|:------------|\n",
        "2021-02-17 | Josiah Wang | First version | \n",
        "2021-02-19 | Josiah Wang | Added equations for EM | \n",
        "2021-11-11 | Josiah Wang | The colour dataset is no longer available online. Updated to point to the conference paper instead. |\n",
        "2023-01-22 | Josiah Wang | Updated link to dataset. |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EbbkgqOgZK_"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "The aim of this lab exercise is to give you some practical experience in implementing unsupervised learning algorithms as discussed in the lectures. This will hopefully help improve your understanding of the algorithms. \n",
        "\n",
        "By the end of this lab exercise, you will have \n",
        "- run some clustering algorithms using scikit-learn's implementation\n",
        "- implemented a working Kernel Density estimator\n",
        "- implemented and fitted a Gaussian distribution model\n",
        "- implemented the Expectation-Maximization (EM) algorithm to fit a Gaussian Mixture Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6JNR5RC1wwk"
      },
      "source": [
        "## A dataset of blue pixels\n",
        "\n",
        "Let us try to improve your understanding of unsupervised learning by tackling a concrete and interesting problem. You will work with a real(-ish) but simple dataset.\n",
        "\n",
        "Background information: A pixel can be represented by three features: R (red), G (green) and B (blue). Each feature can be between 0.0 and 1.0 (inclusive). The combination of these measurements will produce an observable colour on your screen. For example, R=1.0, G=0.6, B=0.0 will result in a pixel that you will perceive as orange.\n",
        "\n",
        "You are provided with a dataset containing examples of \"blue\" pixels. Obviously, \"blue\" can be in different shades and intensity, and what you might perceive as \"blue\" might not be perceived as such by another person (e.g. I would say that is turquoise!)\n",
        "\n",
        "The code below will download a dataset of 100,000 \"blue\" pixels that I have prepared. If you run the code, you will see that the dataset contains different shades of blue (along with some noise).\n",
        "\n",
        "[For those really interested, I obtained these pixels by extracting all pixels containing the word \"blue\" in the label of the dataset [from this paper](https://aclanthology.org/Q15-1008.pdf), and then sampled 100,000 of them at random. The original dataset was actually obtained by asking people on the Web to name the colour shown. More details can be found in [this blog article](https://blog.xkcd.com/2010/05/03/color-survey-results/), in case you find that interesting.]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLq5ievU0ydA"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Download dataset if it does not exist\n",
        "if not os.path.exists(\"blue_rgb_sample.txt\"):\n",
        "    !wget -O blue_rgb_sample.txt https://www.doc.ic.ac.uk/~jwang4/files/intro2ml/blue_rgb_sample.txt\n",
        "\n",
        "x_train = []\n",
        "for line in open(\"blue_rgb_sample.txt\"):\n",
        "    r, g, b = line.strip().split(\",\")\n",
        "    x_train.append(list(map(float, [r, g, b])))\n",
        "\n",
        "x_train = np.array(x_train)  # (100000, 3), each row is an RGB measurement\n",
        "print(x_train.shape)  \n",
        "\n",
        "# Visualise the colour of the first 10,000 pixels.\n",
        "# Reshape the 10,000 pixels into a grid for visualisation\n",
        "# The 3rd dimension contains the R, G and B of each pixel\n",
        "# This allows plt.imshow() to display an RGB image\n",
        "# Feel free to change n_cols and n_rows below to visualise fewer pixels (more clearly)\n",
        "n_cols = 100\n",
        "n_rows = 100\n",
        "plt.imshow(np.reshape(x_train[:n_rows*n_cols, :], (n_rows, n_cols, 3)))\n",
        "plt.show()\n",
        "\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sA-SC7mUU5O8"
      },
      "source": [
        "## Clustering\n",
        "\n",
        "You will have noticed that there are many different shades of blue in this dataset - light blue, dark blue, greenish blue, almost blacks, purples, etc. Let us try to cluster the blues into multiple groups! \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vQo5YuNt9Bz"
      },
      "source": [
        "### Running $K$-means\n",
        "\n",
        "We will not go through implementing $K$-means from scratch as it is relatively straightforward. Of course, feel free to try implementing your own 'vanilla' $K$-means if you would like to gain a deeper understanding.\n",
        "\n",
        "We will instead apply scikit-learn's implementation of the $K$-means algorithm to cluster our blue pixel dataset, and see whether we can discover different variants of blue!\n",
        "\n",
        "The code below will cluster a small subset of the blue pixel dataset into a fixed number of clusters.\n",
        "\n",
        "It will then visualise the clusters by showing some examples from each cluster.\n",
        "\n",
        "Try increasing the number of clusters and observe the effect. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtCRlGyfXqme"
      },
      "source": [
        "!pip install scikit-learn\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# we will work with a small subset of the data to save time and memory\n",
        "# increase it if you like!\n",
        "x_subset = x_train[:1000]\n",
        "\n",
        "n_clusters = 3\n",
        "\n",
        "clusters = KMeans(n_clusters=n_clusters).fit(x_subset)\n",
        "\n",
        "print(clusters.labels_)\n",
        "print(clusters.cluster_centers_)\n",
        "\n",
        "# Visualise cluster centres\n",
        "plt.imshow(np.reshape(clusters.cluster_centers_, (1, -1, 3)))\n",
        "plt.show()\n",
        "\n",
        "# Visualise clusters by showing some examples from each cluster\n",
        "dims = x_subset.shape[1]\n",
        "fig, axes = plt.subplots(1, n_clusters, sharey=True)\n",
        "\n",
        "for k in range(n_clusters):\n",
        "    points = x_subset[clusters.labels_ == k, :]\n",
        "\n",
        "    # reshape pixels into the closest \"square\" grid possible for visualisation\n",
        "    n_rows = np.floor(np.sqrt(len(points))).astype(int)\n",
        "    n_cols = n_rows\n",
        "    axes[k].imshow(np.reshape(points[:n_rows*n_cols, :], (n_rows, n_cols, dims)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBDmXRuWr8Q7"
      },
      "source": [
        "### Elbow method\n",
        "\n",
        "You can also attempt to use the elbow method to determine the optimal number of clusters. Run the code below to get a plot. Unfortunately, here we have an example of the elbow method not working too well. Where is the elbow? I have no definite answer myself. ðŸ˜¢"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNGc6J1boobj"
      },
      "source": [
        "cluster_list = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
        "scores = []\n",
        "for n_clusters in cluster_list:\n",
        "    clusters = KMeans(n_clusters=n_clusters).fit(x_subset)\n",
        "    score = clusters.inertia_  # sum of squares to centroid\n",
        "    print(score)\n",
        "    scores.append(score)\n",
        "\n",
        "plt.plot(cluster_list, scores)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJbEBDWFsfjf"
      },
      "source": [
        "### Other clustering algorithms\n",
        "\n",
        "You can also explore the other clustering algorithms provided by scikit-learn. The [official documentation](https://scikit-learn.org/stable/modules/clustering.html) provides details about many of these clustering algorithms - feel free to read in your own time. You can use the code below to experiment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tw8DqB8tGQP"
      },
      "source": [
        "from sklearn.cluster import SpectralClustering, AgglomerativeClustering\n",
        "\n",
        "n_clusters = 4\n",
        "\n",
        "clusters = SpectralClustering(n_clusters=n_clusters).fit(x_subset)\n",
        "#clusters = AgglomerativeClustering(n_clusters=n_clusters).fit(x_subset)\n",
        "\n",
        "#print(clusters.labels_)\n",
        "\n",
        "# Visualise clusters by showing some examples from each cluster\n",
        "dims = x_subset.shape[1]\n",
        "fig, axes = plt.subplots(1, n_clusters, sharey=True)\n",
        "\n",
        "for k in range(n_clusters):\n",
        "    points = x_subset[clusters.labels_ == k, :]\n",
        "    n_rows = np.floor(np.sqrt(len(points))).astype(int)\n",
        "    n_cols = n_rows\n",
        "    axes[k].imshow(np.reshape(points[:n_rows*n_cols, :], (n_rows, n_cols, dims)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxASa6zpS10p"
      },
      "source": [
        "## Density Estimation\n",
        "\n",
        "We will now tackle another unsupervised learning problem - density estimation. \n",
        "\n",
        "Your task now is to model and discover what a \"blue\" pixel should look like. Your model should be able to estimate how likely a given RGB pixel is to be \"blue\".\n",
        "\n",
        "Intuitively, you model should produce a high density estimate when a pixel is likely to be \"blue\" according to the dataset, and a low density estimate when presented with a red pixel for example. \n",
        "\n",
        "Let us first produce some examples for testing whether our density estimators are working sensibly. Run the code below to generate some hand-picked test examples.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaXWdBtOonKd"
      },
      "source": [
        "# 20 test examples of different colours\n",
        "x_test = np.array([[0.01, 0.05 ,0.05],\n",
        "                    [0.43, 0.63 ,0.01],\n",
        "                    [0.19, 0.02 ,0.33],\n",
        "                    [0.61, 0.52 ,0.70],\n",
        "                    [0.06, 0.21 ,0.43],\n",
        "                    [0.38, 0.63 ,0.05],\n",
        "                    [0.25, 0.04 ,0.31],\n",
        "                    [0.32, 0.59 ,0.20],\n",
        "                    [0.30, 0.09 ,0.01],\n",
        "                    [0.67, 0.26 ,0.71],\n",
        "                    [0.20, 0.58 ,0.27],\n",
        "                    [0.40, 0.38 ,0.24],\n",
        "                    [0.60, 0.73 ,0.07],\n",
        "                    [0.06, 0.08 ,0.57],\n",
        "                    [0.01, 0.64 ,0.43],\n",
        "                    [0.49, 0.58 ,0.66],\n",
        "                    [0.40, 0.11 ,0.20],\n",
        "                    [0.12, 0.55 ,0.01],\n",
        "                    [0.54, 0.06 ,0.59],\n",
        "                    [0.14, 0.34 ,0.69]]) \n",
        "\n",
        "# Visualise the 20 pixels\n",
        "plt.imshow(np.reshape(x_test, (1, -1, 3)))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0PAJ-gWeKSj"
      },
      "source": [
        "### Non-parametric approach: Kernel density estimation\n",
        "\n",
        "We will first take the non-parametric approach and model \"blue\" pixels using Kernel Density Estimation, as described in the lectures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EB_A4QMxpD2"
      },
      "source": [
        "#### Scikit-learn's implementation\n",
        "\n",
        "Let us first try to get a feel of Kernel Density Estimation by using scikit-learn's implementation.\n",
        "Examine the code below, and then run it and observe the output. You should see reasonably high density values for blue-ish instances. \n",
        "\n",
        "Change the bandwidth and observe the effect on the density values. Try both lower and higher bandwidth.\n",
        "\n",
        "You can also experiment with different kernels and distance metrics if you wish. Consult the official [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KernelDensity.html) and [user guide](https://scikit-learn.org/stable/modules/density.html#kernel-density) for these.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwJV1qlctdCE"
      },
      "source": [
        "import sklearn.neighbors\n",
        "\n",
        "kd = sklearn.neighbors.KernelDensity(kernel='gaussian', bandwidth=0.5)\n",
        "kd.fit(x_train)\n",
        "pde = kd.score_samples(x_test)\n",
        "\n",
        "# sklearn's KernelDensity produces log likelihoods\n",
        "# convert it back to likelihood with exp() \n",
        "print(np.exp(pde))\n",
        "\n",
        "# plot test instances\n",
        "plt.imshow(np.reshape(x_test, (1, -1, 3)))\n",
        "plt.show()\n",
        "\n",
        "# plot pde of test instances: lighter = higher density\n",
        "plt.imshow(np.reshape(np.exp(pde), (1, -1)), cmap=\"gray\") \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_LagRXZthUB"
      },
      "source": [
        "#### Your implementation\n",
        "\n",
        "Now, you will implement your own Kernel Density Estimator to gain a better understanding of its details.\n",
        "\n",
        "As in the lectures, we will use a Gaussian kernel.\n",
        "\n",
        "$$\\hat{p}(x) = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{(2 \\pi h^2)^\\frac{D}{2}} \\exp{\\left(- \\frac{||x - x^{(i)}||^2}{2h^2} \\right)}$$\n",
        "\n",
        "where \n",
        "- $h$ is the bandwidth\n",
        "- $||x - x^{(i)}||^2 = (x_1-x_1^{(i)})^2 + (x_2-x_2^{(i)})^2 + \\dotsc + (x_D-x_D^{(i)})^2$ is the sum of squared distance between a test instance $x$ and a training instance $x^{(i)}$ \n",
        "- $D$ is the number of features (3 in our case). \n",
        "\n",
        "Since $\\frac{1}{(2 \\pi h^2)^\\frac{D}{2}}$ is a constant, we can move it to the outside of the summation for our convenience.\n",
        "\n",
        "$$\\hat{p}(x) = \\frac{1}{(\\sqrt{2 \\pi} h)^D} \\frac{1}{N} \\sum_{i=1}^{N}  \\exp{\\left(- \\frac{||x - x^{(i)}||^2}{2h^2} \\right)}$$\n",
        "\n",
        "Since the Kernel Density Estimator is a \"lazy learner\", all the computation will be done at test time. The `.fit()` method of `KernelDensity` below only stores the training examples.\n",
        "\n",
        "Complete the `.pdfs()` method of `KernelDensity` below to compute the density estimate (likelihood) for each given test sample `x_test`. If implemented correctly, you should be able to get the same output as scikit-learn's implementation above.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJAkUDRSk1Wg"
      },
      "source": [
        "class KernelDensity:\n",
        "    def __init__(self, h=1):\n",
        "        self.h = h        \n",
        "        self.x = np.array([])\n",
        "\n",
        "    def fit(self, x):\n",
        "        \"\"\" Store the training examples\n",
        "\n",
        "        Args:\n",
        "            x (np.ndarray): shape (N, D) where \n",
        "                            - N is the number of instances,\n",
        "                            - D is the number of features\n",
        "        \"\"\"        \n",
        "        self.x = x\n",
        "\n",
        "\n",
        "    def pdfs(self, x_test):\n",
        "        \"\"\" Compute the likelihood for a set of test instances\n",
        "\n",
        "        Args:\n",
        "            x_test (np.ndarray): shape (N, D) where \n",
        "                            - N is the number of instances,\n",
        "                            - D is the number of features\n",
        "                            \n",
        "        Returns:\n",
        "            np.ndarray: shape (N, ) containing the density estimate for each N instance\n",
        "        \"\"\"\n",
        "\n",
        "        dim = self.x.shape[1]\n",
        "\n",
        "        # For storing the pairwise distances between test and training instances\n",
        "        distances = np.zeros((len(x_test), len(self.x)))\n",
        "\n",
        "        for (i, x) in enumerate(x_test):\n",
        "            # TODO: Complete this.\n",
        "            # You will only need to implement the bit inside the summation\n",
        "            # i.e. the exp(- ... )\n",
        "\n",
        "            distances[i, :] = ????\n",
        "\n",
        "        density = distances.mean(axis=1) / (np.sqrt(2*np.pi)*self.h)**dim\n",
        "        return density\n",
        "\n",
        "\n",
        "# Test your model\n",
        "my_kd = KernelDensity(h=0.5)\n",
        "my_kd.fit(x_train)\n",
        "pde = my_kd.pdfs(x_test)\n",
        "\n",
        "print(pde)\n",
        "\n",
        "# plot test instances\n",
        "plt.imshow(np.reshape(x_test, (1, -1, 3)))\n",
        "plt.show()\n",
        "\n",
        "# plot pde of test instances: lighter = higher density\n",
        "plt.imshow(np.reshape(pde, (1, -1)), cmap=\"gray\") \n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaS8hhPHm6O5"
      },
      "source": [
        "Here is a larger test set where I generated many possible combinations of RGB values. Run the code below - note that it will be much slower to compute than the previous test set!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyJa4NnMeHTp"
      },
      "source": [
        "# Generate evenly spaced test instances\n",
        "# 0.0 0.0 0.0\n",
        "# 0.0 0.0 0.1\n",
        "# 0.0 0.0 0.2\n",
        "# ...\n",
        "# 1.0 1.0 0.9\n",
        "# 1.0 1.0 1.0\n",
        "\n",
        "points = np.arange(0, 1.01, 0.1)\n",
        "(r_test, g_test, b_test) = np.meshgrid(points, points, points)\n",
        "x_test_large = np.vstack((r_test.ravel(), g_test.ravel(), b_test.ravel())).transpose()\n",
        "\n",
        "pde = my_kd.pdfs(x_test_large)\n",
        "print(pde)\n",
        "\n",
        "plt.imshow(np.reshape(x_test_large, (11, 121, 3)))\n",
        "plt.show()\n",
        "plt.imshow(np.reshape(pde, (11, 121)), cmap='gray')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzzl8oedrh4y"
      },
      "source": [
        "### Parametric approach: Gaussian distribution\n",
        "\n",
        "Next, we will explore the simpler parametric approach to density estimation, by modelling \"blue\" with a multivariate Gaussian distribution. You will assume that there is an 'average' blue, with the variation across the R, G and B measurements captured by a covariance matrix. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f88EYvT32T5M"
      },
      "source": [
        "#### Your implementation\n",
        "\n",
        "Firstly, complete the `.fit()` method of `Gaussian` below. This is simply a matter of computing the mean and the covariance matrix from `x`. \n",
        "\n",
        "$\\mathbf{\\hat{\\mu}} = \\frac{1}{N} \\sum_{i=1}^N \\mathbf{x}^{(i)}$\n",
        "\n",
        "$\\hat{\\Sigma} = \\frac{1}{N} \\sum_{i=1}^N (\\mathbf{x}^{(i)} - \\mathbf{\\hat{\\mu}}) (\\mathbf{x}^{(i)} - \\mathbf{\\hat{\\mu}})^T$\n",
        "\n",
        "Depending on how far you want to go and how comfortable you are with linear algebra, you can compute the mean and covariance from scratch by implementing the equations above. You can also just use NumPy's functions for this to make life easier: `numpy.mean()` or `numpy.cov()`. \n",
        "\n",
        "If using `numpy.cov()`, remember to transpose `x` (it expects a $dim \\times N$ numpy array), and specify `ddof=0` as a keyword argument so that it uses $N$ rather than $N-1$ in the denominator. $N$ is often used in Machine Learning to estimate the (co)variance as derived from the Maximum Likelihood Estimate.\n",
        "\n",
        "Once you are done, then complete the `.pdf()` method of `Gaussian`, which computes the density estimate for *one* instance. Again, you can implement the equation below from scratch, or just use the `.pdf()` method of `scipy.stats.multivariate_normal()` for this. \n",
        "\n",
        "$\\mathcal{N}(\\mathbf{x} | \\mathbf{\\mu}, \\Sigma) = \\frac{1}{\\sqrt{(2 \\pi)^D |\\Sigma|}} \\cdot \\exp^{\\left(- \\frac{1}{2} (\\mathbf{x} - \\mathbf{\\mu})^T \\Sigma^{-1} (\\mathbf{x} - \\mathbf{\\mu})\\right)}$\n",
        "\n",
        "These NumPy functions might be useful if you are implementing the equation from scratch:\n",
        "- `numpy.linalg.inv()` to compute the inverse of a matrix\n",
        "- `numpy.linalg.det()` to compute the determinant of a matrix\n",
        "\n",
        "I will provide an example implementation for both in the solutions. Feel free to study the solutions if you are feeling confused or overwhelmed - you have my permission!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wvBD58prqXw"
      },
      "source": [
        "import scipy.stats\n",
        "\n",
        "class Gaussian:\n",
        "    def __init__(self):\n",
        "        self.mu = None      # (D, ) np.ndarray\n",
        "        self.sigma = None   # (D, D) np.ndarray\n",
        "\n",
        "    def fit(self, x):\n",
        "        \"\"\" Fit the data to a Gaussian distribution\n",
        "\n",
        "        Args:\n",
        "            x (np.ndarray): shape (N, D) where \n",
        "                            - N is the number of instances,\n",
        "                            - D is the number of features\n",
        "        \"\"\"  \n",
        "\n",
        "        ## TODO: Complete these\n",
        "        self.mu = ????\n",
        "        self.sigma = ???? \n",
        "\n",
        "        \n",
        "    def pdf(self, x):\n",
        "        \"\"\" Compute the likelihood for a single test instance\n",
        "\n",
        "        Args:\n",
        "            x (np.ndarray): shape (D, ) where D is the number of features\n",
        "                            \n",
        "        Returns:\n",
        "            float: the density estimate for x\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO: Complete this\n",
        "        return ????\n",
        "\n",
        "\n",
        "    def pdfs(self, x_test):\n",
        "        \"\"\" Compute the likelihood for a set of test instances\n",
        "\n",
        "        Args:\n",
        "            x_test (np.ndarray): shape (N, D) where \n",
        "                            - N is the number of instances,\n",
        "                            - D is the number of features\n",
        "                            \n",
        "        Returns:\n",
        "            np.ndarray: shape (N, ) containing the density estimate for each N instance\n",
        "        \"\"\"        \n",
        "\n",
        "        lik = np.zeros((len(x_test),))\n",
        "        for i in range(len(x_test)):\n",
        "            lik[i] = self.pdf(x_test[i, :])\n",
        "        return lik\n",
        "\n",
        "\n",
        "print(\"Your estimated Gaussian distribution\")\n",
        "model = Gaussian()\n",
        "model.fit(x_train)\n",
        "print(model.mu)\n",
        "print(model.sigma)\n",
        "\n",
        "print(\"\\nTesting your Gaussian distribution\")\n",
        "pde = model.pdfs(x_test)\n",
        "print(pde)\n",
        "\n",
        "# plot the mean\n",
        "plt.imshow(np.reshape(model.mu, (1, -1, 3)))\n",
        "plt.show()\n",
        "\n",
        "# plot test instances\n",
        "plt.imshow(np.reshape(x_test, (1, -1, 3)))\n",
        "plt.show()\n",
        "\n",
        "# plot pde of test instances: lighter = higher density\n",
        "# plotting the log(pde) this time for better visibility \n",
        "plt.imshow(np.reshape(np.log(pde), (1, -1)), cmap=\"gray\") \n",
        "plt.show()\n",
        "\n",
        "\n",
        "# use this to check your expected output\n",
        "print(\"=====================\")\n",
        "print(\"Scikit-learn's output\")\n",
        "import sklearn.mixture\n",
        "sklearn_model = sklearn.mixture.GaussianMixture(n_components=1, \n",
        "                        init_params='random', \n",
        "                        covariance_type='full', \n",
        "                        verbose=0, \n",
        "                        reg_covar=0.)\n",
        "sklearn_model.fit(x_train)\n",
        "print(sklearn_model.means_.squeeze())\n",
        "print(sklearn_model.covariances_.squeeze())\n",
        "log_prob = sklearn_model.score_samples(x_test)\n",
        "print(np.exp(log_prob))\n",
        "\n",
        "plt.imshow(np.reshape(sklearn_model.means_.squeeze(), (1, -1, 3)))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRbGfHXv2OdQ"
      },
      "source": [
        "#### Generating samples\n",
        "\n",
        "Gaussian distributions are **generative models**. Thus, you will also be able to generate samples of \"blue\" pixels from the model you have just fitted. \n",
        "\n",
        "The code below will generate 100 samples of \"blue\" pixels from your Gaussian distribution of \"blue\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7vc4fUq3Jbs"
      },
      "source": [
        "rg = np.random.default_rng(seed=60012)\n",
        "samples = rg.multivariate_normal(model.mu, model.sigma, 100)\n",
        "\n",
        "# keep samples inside the desired range\n",
        "samples[samples < 0] = 0.\n",
        "samples[samples > 1] = 1.\n",
        "\n",
        "plt.imshow(np.reshape(samples, (10, 10, 3)))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9FEG00MvQOm"
      },
      "source": [
        "### Semi-parametric approach: Gaussian Mixture Models\n",
        "\n",
        "Gaussian distributions may be simple and efficient, but may be too simple to model data that is more complex. Kernel density estimators on the other hand are more expressive and can model complex data, but is very slow to compute at test time. Gaussian Mixture Models (GMMs) offer the best of both worlds: the simplicity of Gaussian distributions and the expressiveness of Kernel density estimators. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yT672KbR6Ejf"
      },
      "source": [
        "#### Scikit-learn's implementation\n",
        "\n",
        "We will start with scikit-learn's implementation and try to get the hang of how GMMs work. Run the code below. Experiment with the number of components; the higher the number of components, the more expressive the GMM becomes, but the slower it will take to run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1YGUKQTMi2O"
      },
      "source": [
        "import sklearn.mixture\n",
        "\n",
        "n_components = 3\n",
        "\n",
        "gmm = sklearn.mixture.GaussianMixture(n_components=n_components, \n",
        "                    init_params='random', \n",
        "                    covariance_type='full',\n",
        "                    verbose=2,           # give lots of information\n",
        "                    verbose_interval=1,  # show stats at every 1 iteration\n",
        "                    reg_covar=0.         # this argument is to ensure that the covariance matrix is always positive. Disabled this for now.\n",
        "                    )\n",
        "\n",
        "x_subset = x_train\n",
        "\n",
        "gmm.fit(x_subset)\n",
        "\n",
        "print(gmm.n_iter_)\n",
        "print(gmm.weights_)        \n",
        "print(gmm.means_)\n",
        "print(gmm.covariances_)  \n",
        "\n",
        "# Plot the means of each component\n",
        "plt.imshow(np.reshape(gmm.means_, (1, -1, 3)))\n",
        "plt.show()\n",
        "\n",
        "pde = gmm.score_samples(x_test)\n",
        "print(np.exp(pde))\n",
        "\n",
        "# plot test instances\n",
        "plt.imshow(np.reshape(x_test, (1, -1, 3)))\n",
        "plt.show()\n",
        "\n",
        "# plot pde of test instances: lighter = higher density\n",
        "# plotting the log(pde) this time for better visibility \n",
        "plt.imshow(np.reshape(pde, (1, -1)), cmap=\"gray\") \n",
        "plt.show()\n",
        "\n",
        "# sample 100 examples from the GMM\n",
        "(samples, labels) = gmm.sample(100)\n",
        "samples[samples < 0] = 0.\n",
        "samples[samples > 1] = 1.\n",
        "print(\"\\nSampling 100 instances from the GMM\")\n",
        "plt.imshow(np.reshape(samples, (10, 10, 3)))\n",
        "plt.show()\n",
        "print(\"\\nLabels of the 100 samples\")\n",
        "print(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiiX9z1Q8KsX"
      },
      "source": [
        "#### Your implementation\n",
        "\n",
        "Now it is your turn to implement your own GMM, or more specifically the Expectation Maximization (EM) algorithm to optimise the GMM's parameters. The EM algorithm itself is not difficult, but implementing it correctly from scratch can take some effort. Otherwise scikit-learn's implementation will be more than enough for most practical purposes.\n",
        "\n",
        "The aim of this section is more to improve your understanding of EM by getting your hands dirty, rather than trying to produce the most efficient implementation. The 1D example in tutorial worksheets is already be sufficient for the exam and for improving your understanding. Implementing a multivariate version will bring your understanding to the next level as you will have to start thinking about covariance matrices and all the nuances of linear algebra. So, please do attempt it if/when you have spare time and are up for a challenge! \n",
        "\n",
        "I have provided most of the skeleton code (check out `.fit()` for the main algorithm). You should complete the following methods:\n",
        "- `_e_step()`: To perform an Expectation step\n",
        "- `_m_step()`: To perform a Maximization step\n",
        "- `pdf()`: To compute the GMM likelihood on a single test instance\n",
        "\n",
        "Tip: Make sure you know exactly the dimensions of the matrices that you need before coding. It is mainly about getting the matrices and matrix operations correct. Guessing does not quite work here and will only waste your time! Also beware of [NumPy's array broadcasting](https://numpy.org/doc/stable/user/basics.broadcasting.html), especially if you do not know what you are doing!\n",
        "\n",
        "Here are the equations from the lectures (I swapped the $k$ and $i$ in $r_{ki}$ just to be consistent with the implementation below):\n",
        "- E-step\n",
        "\n",
        "$r_{ki} = \\frac{\\pi_k \\mathcal{N}(\\mathbf{x}^{(i)} | \\mathbf{\\mu}_k, \\Sigma_k)}{\\sum_j^K \\pi_j \\mathcal{N}(\\mathbf{x}^{(i)} | \\mathbf{\\mu}_j, \\Sigma_j)}$\n",
        "\n",
        "- M-step\n",
        "\n",
        "$N_k = \\sum_{i=1}^N r_{ki}$\n",
        "\n",
        "$\\pi_k = \\frac{N_k}{N}$\n",
        "\n",
        "$\\mathbf{\\hat{\\mu}}_k = \\frac{1}{N_k} \\sum_{i=1}^N r_{ki} \\mathbf{x}^{(i)}$\n",
        "\n",
        "$\\mathbf{\\Sigma}_k = \\frac{1}{N_k} \\sum_{i=1}^N r_{ki}\\left( \\mathbf{x}^{(i)} - \\mathbf{\\hat{\\mu}}_k \\right) \\left( \\mathbf{x}^{(i)} - \\mathbf{\\hat{\\mu}}_k \\right)^T$\n",
        "\n",
        "- PDF\n",
        "\n",
        "$p(\\mathbf{x}|\\theta) = \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(\\mathbf{x} | \\mathbf{\\mu_k}, \\Sigma_k)$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SFXZ6O1vWBN"
      },
      "source": [
        "class GaussianMixtureModel:\n",
        "    def __init__(self, k):\n",
        "        self.k = k\n",
        "\n",
        "        # Initialise mixing proportions with uniform weights\n",
        "        self.pi = np.ones((self.k, )) / self.k\n",
        "\n",
        "        # Initialise with empty components. We will reuse our Gaussian class from earlier.\n",
        "        self.components = [Gaussian() for i in range(self.k)] \n",
        "\n",
        "\n",
        "    def _init_params(self, x):\n",
        "        \"\"\" Initialise the parameters of the GMM. \n",
        "        \n",
        "        The mixing proportions (self.pi) have already been initialised in the constructor, \n",
        "        so what's left is to initialise the means and covariances for each component \n",
        "\n",
        "        Args:\n",
        "            x (np.ndarray): shape (N, D) where \n",
        "                            - N is the number of instances,\n",
        "                            - D is the number of features\n",
        "        \"\"\"\n",
        "\n",
        "        # Initialise the mu and sigma for each component\n",
        "        # Select k random instances from x to use as mean for each component\n",
        "        # Initialise the covariance for each component as the covariance of the x\n",
        "\n",
        "        # select k random instances\n",
        "        rg = np.random.default_rng(seed=60012)\n",
        "        random_indices = rg.choice(len(x), self.k)\n",
        "\n",
        "        # compute the covariance of the whole dataset\n",
        "        covar = np.cov(x.T, ddof=0)\n",
        "\n",
        "        # Initialise the mean and covariance for each component\n",
        "        for k in range(self.k):\n",
        "            self.components[k].mu = x[random_indices[k], :]\n",
        "            self.components[k].sigma = covar\n",
        "\n",
        "\n",
        "    def _fit_sklearn_gmm(self, x):\n",
        "        \"\"\" Fit a GMM with scikit-learn to compare with our implementation\n",
        "\n",
        "        Initialise with our own parameters to try to reproduce our own implementation\n",
        "\n",
        "        Note: I found that scikit-learn's convergence check is off by one iteration\n",
        "        compared to my own implementation, but otherwise I managed to reproduce its\n",
        "        output (assuming you can stop its iteration at one step before - \n",
        "        use max_iter below to control this)\n",
        "\n",
        "        Args:\n",
        "            x (np.ndarray): shape (N, D) where \n",
        "                            - N is the number of instances,\n",
        "                            - D is the number of features        \n",
        "        \"\"\"\n",
        "        gmm = sklearn.mixture.GaussianMixture(n_components=n_components, \n",
        "                    init_params='random', \n",
        "                    weights_init=self.pi,\n",
        "                    means_init=[self.components[k].mu for k in range(self.k)],\n",
        "                    precisions_init=[np.linalg.inv(self.components[k].sigma) for k in range(self.k)],\n",
        "                    covariance_type='full',\n",
        "                    verbose=2,\n",
        "                    verbose_interval=1,\n",
        "                    tol=1e-3,      # tolerance for convergence check\n",
        "                    #max_iter=1,   # change this to stop at a specific iteration for inspection purposes\n",
        "                    reg_covar=0.)\n",
        "        gmm.fit(x)\n",
        "\n",
        "        print(gmm.n_iter_)\n",
        "        print(gmm.weights_)\n",
        "        print(gmm.means_)\n",
        "        print(gmm.covariances_)\n",
        "\n",
        "        #print(gmm.score_samples(x_test))\n",
        "\n",
        "\n",
        "    def _compute_average_loglikelihood(self, x):\n",
        "        \"\"\" Compute the average loglikelihood of the data\n",
        "\n",
        "        I am using the average loglikelihood here rather than the sum as in the lectures.\n",
        "        This is so that I can compare it to scikit-learn's average loglikelihood \n",
        "        \"\"\"\n",
        "        \n",
        "        # Compute unweighted likelihoods for x per component\n",
        "        lik = self._compute_component_likelihood(x)\n",
        "\n",
        "        # Weighted sum across components, and sum the loglikelihood across N instances\n",
        "        loglik = 0\n",
        "        for i in range(len(x)):\n",
        "            loglik += np.log(np.dot(self.pi, lik[:, i]))\n",
        "\n",
        "        # Return the average log likelihood\n",
        "        return loglik / len(x)\n",
        "\n",
        "\n",
        "    def _compute_component_likelihood(self, x):\n",
        "        \"\"\" Compute the unweighted per-component likelihood of the data\n",
        "\n",
        "        Args:\n",
        "            x (np.ndarray): shape (N, D) where \n",
        "                            - N is the number of instances,\n",
        "                            - D is the number of features\n",
        "                            \n",
        "        Returns:\n",
        "            np.ndarray: shape (K, N) containing the likelihood for each component and each instance\n",
        "        \"\"\"\n",
        "        \n",
        "        lik = np.zeros((self.k, len(x)))  # k x N\n",
        "        for k in range(self.k):\n",
        "            lik[k, :] = self.components[k].pdfs(x)\n",
        "\n",
        "        return lik\n",
        "\n",
        "\n",
        "    def _e_step(self, x):\n",
        "        \"\"\" Perform a single E-step of EM\n",
        "\n",
        "        Compute and return the responsibilities per component per instance\n",
        "\n",
        "        Args:\n",
        "            x (np.ndarray): shape (N, D) where \n",
        "                            - N is the number of instances,\n",
        "                            - D is the number of features\n",
        "                            \n",
        "        Returns:\n",
        "            np.ndarray: shape (K, N) containing the responsibilities for each component and each instance\n",
        "        \"\"\"\n",
        "\n",
        "        # Compute the (unweighted) likelihood for each component, for each instance\n",
        "        # this will be the N(x|mu_k, sigma_k) for each component\n",
        "        # You may use this in your calculations\n",
        "        lik = self._compute_component_likelihood(x)\n",
        "\n",
        "        # TODO: Compute posterior/responsibilities\n",
        "        r = ????\n",
        "\n",
        "        return r\n",
        "\n",
        "\n",
        "    def _m_step(self, x, r):\n",
        "        \"\"\" Perform a single M-step of EM\n",
        "\n",
        "        Update the parameters (weights, means, convariances) of the GMM\n",
        "\n",
        "        Args:\n",
        "            x (np.ndarray): shape (N, D) where \n",
        "                            - N is the number of instances,\n",
        "                            - D is the number of features\n",
        "            r (np.ndarray): shape (K, N) containing the responsibilities for each component and each instance\n",
        "        \"\"\"\n",
        "\n",
        "        # Sum the responsibilities for each component\n",
        "        # You may use this in your calculations\n",
        "        r_sum = np.sum(r, axis=1)    # (k, )\n",
        "\n",
        "        ## TODO: Update mixing proportions\n",
        "        self.pi = ????\n",
        "        #print(self.pi)\n",
        "\n",
        "        for k in range(self.k):\n",
        "            ## TODO: Update the mean and covariance for each component\n",
        "\n",
        "            self.components[k].mu = ????\n",
        "            self.components[k].sigma = ????\n",
        "\n",
        "        #for k in range(self.k):\n",
        "        #    print(self.components[k].mu)\n",
        "        #for k in range(self.k):\n",
        "        #    print(self.components[k].sigma)\n",
        "\n",
        "\n",
        "    def fit(self, x):\n",
        "        # STEP 1: Initialise parameters for GMM\n",
        "        print(\"Initialising the GMM parameters...\")\n",
        "        self._init_params(x)\n",
        "        \n",
        "        # For debugging purposes\n",
        "        # Run scikit-learn's GMM/EM by initialising with our initial parameters\n",
        "        # to check that our implementation is correct\n",
        "        print(\"Fitting data to scikit-learn's GMM\")\n",
        "        self._fit_sklearn_gmm(x)\n",
        "\n",
        "        # Compute the log likelihood for convergence checking\n",
        "        lik_prev = np.inf        \n",
        "        lik_current = self._compute_average_loglikelihood(x)\n",
        "        print(f\"Current loglikelihood: {lik_current:.5f}\")\n",
        "       \n",
        "        iter = 0\n",
        "        tolerance = 1e-3  # for convergence checking\n",
        "        converged = False\n",
        "\n",
        "        while not converged:\n",
        "            iter = iter + 1\n",
        "\n",
        "            lik_prev = lik_current  # keep track track previous loglikelihood\n",
        "\n",
        "            print(f\"\\nIteration: {iter}\")\n",
        "\n",
        "            # STEP 2: Expectation step\n",
        "            r = self._e_step(x)\n",
        "\n",
        "            # STEP 3: Maximization step\n",
        "            self._m_step(x, r)\n",
        "\n",
        "            # STEP 4: Convergence check\n",
        "            lik_current = self._compute_average_loglikelihood(x)\n",
        "            delta = np.abs(lik_current - lik_prev)\n",
        "            if delta <= tolerance:\n",
        "                converged = True\n",
        "            print(f\"LL={lik_current:.5f}; change={delta:.5f}\")\n",
        "\n",
        "     \n",
        "    def pdf(self, x):\n",
        "        \"\"\" Compute the likelihood for a single test instance\n",
        "\n",
        "        Args:\n",
        "            x (np.ndarray): shape (D, ) where D is the number of features\n",
        "                            \n",
        "        Returns:\n",
        "            float: the density estimate for x\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO: Complete this to return the likelihood for an instance x\n",
        "\n",
        "        return ????\n",
        "\n",
        "\n",
        "    def pdfs(self, x_test):\n",
        "        \"\"\" Compute the likelihood for a set of test instances\n",
        "\n",
        "        Args:\n",
        "            x_test (np.ndarray): shape (N, D) where \n",
        "                            - N is the number of instances,\n",
        "                            - D is the number of features\n",
        "                            \n",
        "        Returns:\n",
        "            np.ndarray: shape (N, ) containing the density estimate for each N instance\n",
        "        \"\"\"        \n",
        "\n",
        "        lik = np.zeros((len(x_test),))\n",
        "        for i in range(len(x_test)):\n",
        "            lik[i] = self.pdf(x_test[i, :])\n",
        "        return lik\n",
        "\n",
        "\n",
        "    def sample(self, n=1):\n",
        "        \"\"\" Generate n samples from the GMM\n",
        "\n",
        "        Args:\n",
        "            n (int): number of samples to generate\n",
        "                            \n",
        "        Returns: \n",
        "            tuple: (samples, labels) where\n",
        "                    - samples is a np.ndarray shaped (n, dims), n samples, each with dims features\n",
        "                    - labels is a np.ndarray shaped (n, ), the component label for each sample\n",
        "        \"\"\"       \n",
        "\n",
        "        rg = np.random.default_rng(seed=60012)\n",
        "\n",
        "        # choose n components according to the mixing distribution\n",
        "        label_distribution = rg.multinomial(n, self.pi)\n",
        "\n",
        "        # draw a sample for each selected component\n",
        "        samples = np.zeros((n, len(self.components[0].mu)))\n",
        "        labels = np.zeros((n, ))\n",
        "        i = 0\n",
        "        for (k, count) in enumerate(label_distribution):\n",
        "            out = rg.multivariate_normal(self.components[k].mu, \n",
        "                                        self.components[k].sigma, \n",
        "                                        size=count)\n",
        "            samples[i:i+count, :] = out\n",
        "            labels[i:i+count] = k\n",
        "            i = i + count\n",
        "\n",
        "        # clip boundary cases\n",
        "        samples[samples < 0] = 0.\n",
        "        samples[samples > 1] = 1.\n",
        "        return (samples, labels)\n",
        "\n",
        "\n",
        "n_components = 3\n",
        "model = GaussianMixtureModel(n_components)\n",
        "model.fit(x_train[:1000, :])\n",
        "\n",
        "print(\"\\nFinal model:\")\n",
        "print(f\"Weights: {model.pi}\")\n",
        "print(f\"\\nComponent means: \")\n",
        "means = np.array([component.mu for component in model.components])\n",
        "print(means)\n",
        "print(f\"\\nComponent covariances:\")\n",
        "covariances = np.array([component.sigma for component in model.components])\n",
        "print(covariances)\n",
        "\n",
        "# plot mean of each component\n",
        "plt.imshow(np.reshape(means, (1, -1, 3)))\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nTesting your GMM\")\n",
        "pde = model.pdfs(x_test)\n",
        "print(np.log(pde))\n",
        "\n",
        "# plot test instances\n",
        "plt.imshow(np.reshape(x_test, (1, -1, 3)))\n",
        "plt.show()\n",
        "\n",
        "# plot pde of test instances: lighter = higher density\n",
        "# plotting the log(pde) this time for better visibility \n",
        "plt.imshow(np.reshape(np.log(pde), (1, -1)), cmap=\"gray\") \n",
        "plt.show()\n",
        "\n",
        "(samples, labels) = model.sample(100)\n",
        "print(\"\\nSampling 100 instances from the GMM\")\n",
        "plt.imshow(np.reshape(samples, (10, 10, 3)))\n",
        "plt.show()\n",
        "print(\"\\nLabels of the 100 samples\")\n",
        "print(labels)\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNMajcyH7Vv5"
      },
      "source": [
        "## Summary\n",
        "\n",
        "Phew! I think that's enough linear algebra and matrix manipulation! You have applied unsupervised learning to a simple but concrete problem, for clustering and density estimation. You may have even implemented your own density estimation algorithms and models. \n",
        "\n",
        "Hopefully you have managed to gain some practical experience on unsupervised learning to aid your understanding on the topic!\n"
      ]
    }
  ]
}